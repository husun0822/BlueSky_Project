{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import sys\n",
    "from functools import partial\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib as mpl\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from skimage.filters import scharr\n",
    "import itertools\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIL():\n",
    "    '''This class creates a PIL detector which can be called for fitting PIL on any Bz image or an entire hdf5 file.\n",
    "        \n",
    "       Summary on the class \"PIL\":\n",
    "       \n",
    "       For each image of the vertical component of the magnetic field, namely the image of \"Bz\", we want to find the \n",
    "       contour of Bz==0 that separates the strong positive magnetic polars and the associated negative polars, which \n",
    "       is called the Polarity Inversion Line(PIL). Physicists are interested in the local magnetic field features \n",
    "       around the PIL.\n",
    "       \n",
    "       In a pixelized image, the pixels that consist of the PIL should be those pixels that have a large Bz gradient in \n",
    "       its neighborhood. And finding these points is analogous to the edge detection task in the image processing field.\n",
    "       So by applying an edge detection filter upon each Bz image, we could find the candidate pixels of the PIL. And by \n",
    "       further applying a clustering algorithm on the candidate pixels, we are able to locate several PIL segments in the\n",
    "       image. Finally, one could retain, as many as one wanted, the PILs with large average gradients for future purposes.\n",
    "       \n",
    "       The class PIL is a parameterized PIL detector that combines the data preparation, edge detection, clustering and \n",
    "       parallelized training, and visualization procedures. It can be applied to a piece of image in numpy ndarray form, \n",
    "       and even an entire hdf5 file with rather fast implementation.\n",
    "       \n",
    "       There are generally two modes for the PIL algorithm, the image_mode and the video_mode(global_mode).\n",
    "       image mode: PILs are detected based on each image's data. The user can view the image mode as passing an image into\n",
    "                   the .fit() method, and PILs's location and gradient for that image is the output.\n",
    "       \n",
    "       video mode: PILs are detected based on the whole video data. The user can view the video mode as passing a video into\n",
    "                   the .fit() method, and PILs's location and gradient for each image of the video is the output. The \n",
    "                   key difference with the image mode is that, in the video mode a pixel is considered as a \"strong\" pixel\n",
    "                   only if the vertical component's magnitude of the pixel is large compared with all pixels in the whole \n",
    "                   video. In the image mode, a \"strong\" pixel is considered as \"strong\" as long as the pixel's magnitude is\n",
    "                   large compared with all pixels in the image. So the video mode considers all global strong polars while\n",
    "                   the image mode considers all local strong polars.\n",
    "                   \n",
    "                   The advantage of image mode is that the program runs generally quicker. The video mode, on the other hand,\n",
    "                   take the time consistency of PILs into consideration and picks up the video-level strongest PILs, so that \n",
    "                   only those time points when very strong positive and negative polar coexist will have PILs.\n",
    "                   \n",
    "                   In the following methods, nearly all parameters have the same meaning under both the image mode and the\n",
    "                   video mode. The default mode for the package is the image mode. When the parameter is being used under the\n",
    "                   video mode, the documentation will provide extra explanation to show the meaning of the parameter in the \n",
    "                   video mode.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self,edge_retain=0.005,polar_threshold=0.50,radius=12,min_samples=5,mode='image'):\n",
    "        '''Initialize the PIL detector\n",
    "        \n",
    "           Params:\n",
    "           edge_retain: The fraction of candidate PIL pixels of the input image. Range from 0 to 1. Float number.\n",
    "                        What fraction of the pixels of the whole input image can be considered as a candidate for PIL. \n",
    "                        The larger the amount is, the more pixels there will be for the PIL. The default amount is 0.005\n",
    "                        which corresponds to 0.5% of all pixels in the input image. These pixels are the pixels with the \n",
    "                        largest local gradient. By specifying the edge_retain to be too large, we may include many irrelevant\n",
    "                        pixels when drawing the PIL. By setting the parameter to be too small, we may lose some pixels on\n",
    "                        the true PIL. The best way to tune the parameter is to use the .visualize() method to check visually. \n",
    "                        Generally speaking, each image won't have more than 1% of its pixels as PIL.\n",
    "                        \n",
    "           polar_threshold: The strong polar threshold. Range from 0 to 1. Float number.\n",
    "                            The quantile of the magnitude of the vertical component above which a pixel is considered as \n",
    "                            a strong polar. Basically, each image is decomposed into a positive image and a complementing\n",
    "                            negative image. In a positive image, all negative image pixels are coerced to 0, and vice versa.\n",
    "                            In the positive image, for instance, all pixels' values are ranked from the smallest to the \n",
    "                            largest, and all the pixels that are ranking at the quantile no greater than polar_threshold are\n",
    "                            further coerced to 0. The same procedure applies to the negative image. And the positive and \n",
    "                            negative image are put back together as a sparser image, upon which the edge detector will be\n",
    "                            applied. Setting the parameter too small can lead to many weaker polars being considered when\n",
    "                            drawing the PIL.\n",
    "                            \n",
    "           radius: The maximum pixel distance between two pixels that make the pixels being considered as on the same PIL. Positive Integer. \n",
    "                   The larger the quantity, the less PIL segments will be drawn in the image, but maybe two originally \n",
    "                   separated PILs will be considered mistakenly as a single PIL.\n",
    "           \n",
    "           min_samples: The minimum number of pixels that a PIL segment should contain. Positive Integer.\n",
    "                        For a set of pixels to be considered as a PIL, one need at least min_samples in the set. By setting\n",
    "                        the parameter to be too large, we may mistakenly overlook those shorter PIL. On the contrast, we may\n",
    "                        include too many noisy PILs that are not ideal.\n",
    "           \n",
    "           mode: The mode of the PIL detector. Default is 'image', alternative is 'video'.\n",
    "                 See the summary of the PIL() class above for the information on image mode and video mode.\n",
    "        '''\n",
    "        # take records for each of the parameter\n",
    "        self.edge_retain = edge_retain\n",
    "        self.polar_threshold = polar_threshold*100\n",
    "        self.radius = radius\n",
    "        self.min_samples = min_samples\n",
    "        self.window_size = 5 # see .data_preparation() method for reference\n",
    "        self.DBSCAN = DBSCAN(eps=self.radius,min_samples=self.min_samples) # set up the DBSCAN cluster operator for pixels clustering\n",
    "        self.PIL_segment = defaultdict(dict) # the default PIL result for an image is an empty dictionary\n",
    "        \n",
    "        # image mode threshold\n",
    "        self.pos_threshold = 0\n",
    "        self.neg_threshold = 0\n",
    "        \n",
    "        # video mode threshold\n",
    "        self.pos_threshold_video = 0\n",
    "        self.neg_threshold_video = 0\n",
    "        \n",
    "        if mode=='image':\n",
    "            self.global_mode=False # an indicator for distringuishing whether the code is run on a single image or on a whole video\n",
    "        elif mode=='video':\n",
    "            self.global_mode=True\n",
    "        else:\n",
    "            raise Exception('Unknown mode for the PIL algorithm')\n",
    "        \n",
    "        self.invalid_image=False # a flag indicating whether the image can be processed\n",
    "        self.invalid_video=False # a flag indicating whether the video can be processed\n",
    "        # images can be invalid due to:\n",
    "        # a. no positive or no negative pixels\n",
    "        # b. more than 1/5 of the pixels are NaN\n",
    "        \n",
    "        # videos can be invalid due to:\n",
    "        # a. no positive or no negative pixels throughout the whole video\n",
    "        # b. more than 1/3 of the pixels in the whole video are NaN, which is surely rare\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def Image_Decompose(self,image):\n",
    "        '''Data preparation method, a submethod for .data_preparation() method. The method calculates the image's \"strong\"\n",
    "           polar threshold.\n",
    "           \n",
    "           Params:\n",
    "           image: Input image. Numpy array.\n",
    "                  Shape = [height,width]\n",
    "        '''\n",
    "        image = np.nan_to_num(image) # fill the NaN value in an image as 0\n",
    "        \n",
    "        pos_weight = np.where(image>1e-6,image,0) # create a positive image component\n",
    "        neg_weight = np.where(image<-1e-6,image,0) # create a negative image component\n",
    "        \n",
    "        pos_weight, neg_weight = np.ndarray.flatten(pos_weight), np.ndarray.flatten(neg_weight) # collapse both image into 1-D\n",
    "        \n",
    "        pos_weight = pos_weight[np.nonzero(pos_weight)] # remove all 0s in the pos_weight\n",
    "        neg_weight = neg_weight[np.nonzero(neg_weight)] # remove all 0s in the neg_weight\n",
    "    \n",
    "        if pos_weight.shape[0]==0 or neg_weight.shape[0]==0:\n",
    "            self.invalid_image = True # if there is either no positive or no negative pixel, we consider it as an invalid image\n",
    "            warnings.warn(message=\"Missing positive or negative polar in the image.\")\n",
    "            \n",
    "        if (pos_weight.shape[0]+neg_weight.shape[0])<(image.shape[0]*image.shape[1])*(1/5):\n",
    "            self.invalid_image = True # if there are more than one-fifth NaN pixels in the image, do not process the image\n",
    "            warnings.warn(message=\"Too many missing values in the image.\")\n",
    "            \n",
    "        if self.invalid_image==False and self.global_mode==False:\n",
    "            self.pos_threshold = np.percentile(pos_weight, q=self.polar_threshold)\n",
    "            self.neg_threshold = np.percentile(np.abs(neg_weight), q=self.polar_threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def video_data_decompose(self,video):\n",
    "        ''' This function is used to decompose a video into a positive video and a complementing negative video. \n",
    "            For positive video, each frame only contains the positive pixels from that frame in the original video, \n",
    "            with all other pixels coerced to 0. The method calculates the video's \"strong\" polar threshold.\n",
    "            \n",
    "            Params:\n",
    "            video: Input video. Numpy array.\n",
    "                   Shape = [video_len,height,width]\n",
    "            \n",
    "        '''\n",
    "        video = np.nan_to_num(video) # fill the NaN value in the video as 0\n",
    "              \n",
    "        pos_weight = np.where(video>1e-6,video,0) # create a positive video component\n",
    "        neg_weight = np.where(video<-1e-6,video,0) # create a negative image component\n",
    "        \n",
    "        pos_weight, neg_weight = np.ndarray.flatten(pos_weight), np.ndarray.flatten(neg_weight) # collapse both image into 1-D\n",
    "        \n",
    "        pos_weight = pos_weight[np.nonzero(pos_weight)] # remove all 0s in the pos_weight\n",
    "        neg_weight = neg_weight[np.nonzero(neg_weight)] # remove all 0s in the neg_weight\n",
    "    \n",
    "        if pos_weight.shape[0]==0 or neg_weight.shape[0]==0:\n",
    "            self.invalid_video = True # if there is either no positive or no negative pixel, we consider it as an invalid video\n",
    "            warnings.warn(message=\"Missing positive or negative polar in the video.\")\n",
    "            \n",
    "        if (pos_weight.shape[0]+neg_weight.shape[0])<(video.shape[0]*video.shape[1]*video.shape[2])*(1/3):\n",
    "            self.invalid_video = True # if there are more than one-third NaN pixels in the video, do not process the image\n",
    "            warnings.warn(message=\"Too many missing values in the video.\")\n",
    "                \n",
    "        if self.invalid_video==False:\n",
    "            self.pos_threshold_video = np.percentile(pos_weight, q=self.polar_threshold)\n",
    "            self.neg_threshold_video = np.percentile(np.abs(neg_weight), q=self.polar_threshold)\n",
    "            \n",
    "        return self.pos_threshold_video, self.neg_threshold_video\n",
    "        \n",
    "        \n",
    "    \n",
    "    def data_preparation(self,image,window_size=5):\n",
    "        '''Data preparation method, a submethod for the .fit() method. Can be updated with maxpooling2D layer\n",
    "           from the Keras package. Update may apply later.\n",
    "        \n",
    "           Params:\n",
    "           image: Input image. Numpy array.\n",
    "                  Shape = [height,width]\n",
    "           \n",
    "           window_size: The range of a pixel's neighborhood. Odd number integer. No smaller than 3.\n",
    "                        For each pixel in the input image, we consider a window_size*window_size, 5*5 by default, subimage with\n",
    "                        the pixel of interest being at the center as the pixel's neighborhood in the image.\n",
    "                        \n",
    "           Output:\n",
    "           image_new: Output image ready for edge detection. Numpy array.\n",
    "                      Shape = [height, width], same as the input image.\n",
    "                      \n",
    "                      For each pixel in the original image, one consider the local maximum and local minimum of the pixel within\n",
    "                      its neighborhood. If both the local maximum and minimum passed the quantile threshold sepecified by \n",
    "                      self.polar_threshold in the corresponding positive and negative image, the pixel's value is retained,\n",
    "                      otherwise the pixel is put to zero.\n",
    "                      \n",
    "                      As a result, the output image only contains those pixels with both a strong positive and a strong negative\n",
    "                      polar within its neighborhood. This helps exclude those pixels with a strong positive/negative polar but \n",
    "                      no associative polar of the opposite polarity. So these pixels in the output image is a superset of PIL pixels.\n",
    "        '''\n",
    "                   \n",
    "        image = np.nan_to_num(image) # fill all NaN in the image as 0\n",
    "        self.window_size = window_size # store the window_size attribute        \n",
    "        \n",
    "        if self.invalid_image==False:\n",
    "    \n",
    "            height = image.shape[0]\n",
    "            width = image.shape[1]\n",
    "            \n",
    "            if self.global_mode==False:\n",
    "                pos_threshold = self.pos_threshold \n",
    "                neg_threshold = self.neg_threshold\n",
    "            else:\n",
    "                pos_threshold = self.pos_threshold_video\n",
    "                neg_threshold = self.neg_threshold_video\n",
    "            \n",
    "            pad_size = int((window_size-1)/2)\n",
    "            image_padded = np.pad(image,((pad_size,pad_size),(pad_size,pad_size)),'constant') # pad the image to make sure the output image has the same shape as the input image\n",
    "    \n",
    "            image_max = np.zeros((height,width)) # an image with each pixel storing its local maximum\n",
    "            image_min = np.zeros((height,width)) # an image with each pixel storing its local minimum\n",
    "            image_new = np.zeros((height,width)) # output image\n",
    "    \n",
    "    \n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    x = i+pad_size\n",
    "                    y = j+pad_size\n",
    "            \n",
    "                    window = image_padded[(x-pad_size):(x+pad_size+1),(y-pad_size):(y+pad_size+1)] # the neighborhood subimage for each pixel\n",
    "                    maximum = np.amax(window) # find the local maximum\n",
    "                    minimum = np.amin(window) # find the local minimum\n",
    "            \n",
    "                    image_max[i,j] = maximum\n",
    "                    image_min[i,j] = minimum\n",
    "                    if maximum>pos_threshold and minimum<-(neg_threshold): # retains the pixel value only if it has both a strong positive polar and a strong negative polar in its neighborhood subimage\n",
    "                        image_new[i,j] = image[i,j]\n",
    "                \n",
    "            return image_new\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def fit(self,image,select=1,window_size=5):\n",
    "        ''' Fit the whole PIL pipeline on the input image. Including the edge detection, clustering and filtering steps. \n",
    "            Given an input image, the fit method find all PIL pixels, and return a dictionary containing information of \n",
    "            the selected PILs with the largest average local magnetic field gradient. Each PIL's information includes its \n",
    "            corresponding pixels and average local gradients. \n",
    "            \n",
    "            Params:\n",
    "            image: Input image. Numpy array.\n",
    "                   Shape = [height,width]\n",
    "            \n",
    "                   Original image upon which one wants to locate several PILs.\n",
    "                               \n",
    "            select: Number of PILs to record. Positive Integer. Default is 1.\n",
    "                    \n",
    "                    Out of all PIL segments detected, how many of them are going to be recorded. All PIL segments are ranked \n",
    "                    according to their pixel-average local magnetic gradient, and only the PILs with the largest average gradient\n",
    "                    are recorded.\n",
    "            \n",
    "            window_size: The range of a pixel's neighborhood. Odd number integer. No smaller than 3.\n",
    "                         See description in method .data_preparation()\n",
    "            \n",
    "            Output: \n",
    "            PIL_segment: Information of the pixels of the selected PIL segments. Dictionary.\n",
    "                       \n",
    "                       The output is a dictionary: PIL_segment[segment_label]['coor'] = array of the coordinates of all pixels of the segment \n",
    "                                                   PIL_segment[segment_label]['gradient'] = average gradient of the PIL segment\n",
    "                                                   \n",
    "                       PIL segment labels are ranked in a descending order based on the average gradient.\n",
    "        '''\n",
    "        self.Image_Decompose(image) # check the validity of the image and update the threshold under image mode\n",
    "        image = self.data_preparation(image,window_size=window_size) # only retain the pixels with both a strong positive and a strong negative polar in the neighborhood\n",
    "        \n",
    "        if self.invalid_image==False:\n",
    "            edge = scharr(image) # edge detection \n",
    "        \n",
    "            threshold = np.percentile(edge,q=100*(1-self.edge_retain)) # calculates the top quantile for local gradients\n",
    "    \n",
    "            # take records of the coordinates of all pixels that have a very large local graidents.\n",
    "            edge = np.where(edge>threshold,edge,0)\n",
    "            pixel_coor = np.transpose(np.nonzero(edge)) # these are the PIL candidates' coordinates\n",
    "            pixel_weight = list(map(lambda x: edge[x[0],x[1]],pixel_coor)) # the corresponding local gradient values for the PIL candidates\n",
    "        \n",
    "            thecluster = self.DBSCAN.fit_predict(X=pixel_coor) # cluster the PIL candidates into possibly several PIL segments.\n",
    "    \n",
    "            # now we are going to select the top few clusters with the largest average gradient\n",
    "            N = len(set(thecluster)) # number of PIL segments\n",
    "            cluster_coor = defaultdict(list) # a dictionary: cluster_coor[PIL_segment_label] = list of pixels of the PIL segment\n",
    "            cluster_gradient = defaultdict(list) # a dictionary: cluster_gradient[PIL_segment_label] = list of pixels' gradient of the PIL segment\n",
    "        \n",
    "            whole_data = list(zip(pixel_coor,pixel_weight,thecluster))\n",
    "            new_data = []\n",
    "        \n",
    "            # since there might be some PIL candidate pixels that do not even form a PIL segment, we should exclude all of these noisy points\n",
    "            if (-1) in thecluster:\n",
    "                N = N-1 \n",
    "            for item in whole_data:\n",
    "                if item[2]!=-1:\n",
    "                    new_data.append(item)\n",
    "                    \n",
    "            if N!=0: # if all candidate points are noises, then do nothing\n",
    "\n",
    "                # record the coordinates information and gradient information for each of the PIL segment, namely each cluster\n",
    "                for c in range(N):\n",
    "                    cluster_coor[c] = [item[0] for item in new_data if item[2]==c]\n",
    "                    cluster_gradient[c] = [item[1] for item in new_data if item[2]==c]\n",
    "        \n",
    "                # calculate the segment average gradient and rank all PILs, select the top few PIL segments for final output\n",
    "                cluster_ave_gradient = [np.average(np.array(cluster_gradient[c])) for c in cluster_gradient]\n",
    "        \n",
    "                if len(cluster_ave_gradient)>select:\n",
    "                    top = np.argsort(cluster_ave_gradient)[-(select):] \n",
    "                else:\n",
    "                    top = np.argsort(cluster_ave_gradient)\n",
    "        \n",
    "                # record the coordinate information for the selected PIL segments\n",
    "                PIL_segment = defaultdict(dict)\n",
    "                order = list(reversed(top))\n",
    "        \n",
    "                for c in range(len(top)):\n",
    "                    k = order[c]\n",
    "                    coor = cluster_coor[k] # the list of coordinate of one of the top PIL segment\n",
    "                    PIL_segment[c]['coor'] = np.array(coor)\n",
    "                    PIL_segment[c]['weight'] = cluster_ave_gradient[k]\n",
    "    \n",
    "                self.PIL_segment = PIL_segment # PIL results\n",
    "                self.N = N # total PIL segments found\n",
    "                return self.PIL_segment\n",
    "            \n",
    "            else:\n",
    "                print(\"No PIL in the image!\")\n",
    "                return self.PIL_segment # return an empty dictionary\n",
    "        else:\n",
    "            return self.PIL_segment # if the image is not valid, only return an emtpy dictionary\n",
    "    \n",
    "    def visualize(self,image,select=3,window_size=5,savefigure=False,figname='PIL'):\n",
    "        ''' A visualization method for showing the original data, the data input, and the PIL.\n",
    "            \n",
    "            Params:\n",
    "            image: the original data image.\n",
    "            \n",
    "            select: the number of PIL segments to show.\n",
    "            \n",
    "            window_size: the range of a pixel's neighborhood, see documentation on .data_praparation() method for reference\n",
    "            \n",
    "            Output:\n",
    "            A 1*3 figure with left panel being the grayscale heatmap for the original data, the middle panel being the \n",
    "            result of the .data_preparation() method, which is exactly the data input into the .fit() method, and the \n",
    "            right panel being the corresponding PILs founded. The parameters for the .fit() method should be set up \n",
    "            when a PIL() object is being created.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        image_new = self.data_preparation(image,window_size) # calculate the prepared image\n",
    "        \n",
    "        if self.invalid_image==False:       \n",
    "            PILs = self.fit(image,select=select,window_size=window_size) # the PIL segments\n",
    "            points = [] # the coordinates for all PIL points\n",
    "            weights = [] # the corresponding average gradient of the PIL segment to which the PIl point belongs to\n",
    "            PIL_image = np.zeros_like(image)\n",
    "            \n",
    "            # we are going to mark each PIL segments with a color that corresponds to its average gradient so as to show \n",
    "            # the relative strength of each PIL segment\n",
    "            if len(PILs)>0:# if there is any PIL segments founded\n",
    "                for k in PILs:\n",
    "                    coors = PILs[k]['coor']\n",
    "                    weight = PILs[k]['weight']\n",
    "                    \n",
    "                    for p in range(coors.shape[0]):\n",
    "                        points.append(coors[p])\n",
    "                        weights.append(weight)\n",
    "                for p in range(len(points)):\n",
    "                    thepoint = points[p]\n",
    "                    PIL_image[thepoint[0],thepoint[1]] = weights[p]\n",
    "            \n",
    "            \n",
    "            fig = plt.subplots(nrows=1, ncols=3)\n",
    "            cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True) # use the grayscale color map\n",
    "\n",
    "            plt.subplot(1, 3, 1)\n",
    "            seaborn.heatmap(image, center=0, cmap=cmap,cbar=False,xticklabels=False, yticklabels=False)\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            seaborn.heatmap(image_new, center=0, cmap=cmap,cbar=False,xticklabels=False, yticklabels=False)\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            seaborn.heatmap(PIL_image, center=0, cmap=cmap,cbar=False,xticklabels=False, yticklabels=False)\n",
    "                \n",
    "            if savefigure==True:\n",
    "                plt.savefig(figname+'.pdf')\n",
    "            else:\n",
    "                plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_fit(image,edge_retain=0.003,polar_threshold=0.50,radius=12,min_samples=5,select=1,\n",
    "            window_size=5,image_index=-1,mode='image',global_pos=0,global_neg=0):\n",
    "    ''' An explicit function, instead of a class method, for fitting PIL over an image. Basically a class method wrapper.\n",
    "        \n",
    "        Params:\n",
    "        image, edge_retain, polar_threshold, radius, min_samples, select, window_size: see the PIL class documentation.\n",
    "        \n",
    "        image_index: the index of the image processed by this function. Useful if the multiprocessing of several frames is \n",
    "                     done asynchronously.\n",
    "        \n",
    "    '''\n",
    "    PIL_detector = PIL(edge_retain=edge_retain,polar_threshold=polar_threshold,radius=radius,min_samples=min_samples,mode=mode)\n",
    "    if mode=='video':\n",
    "        PIL_detector.pos_threshold_video = global_pos\n",
    "        PIL_detector.neg_threshold_video = global_neg\n",
    "    \n",
    "    result = PIL_detector.fit(image,select=select,window_size=window_size)\n",
    "    \n",
    "    return result, image_index\n",
    "\n",
    "def PIL_fit_parallel(f,mode='image',core=8,global_threshold=0.3,edge_retain=0.003,polar_threshold=0.50,\n",
    "                     radius=12,min_samples=5,select=1,window_size=5,filename='Nil'):\n",
    "    ''' Fit the PIL pipeline over all images in a numpy array file f/ hdf5 file f. For current python environment,\n",
    "            it is unsatisfactory that the class method .fit() cannot be made parallelized as a separate class method\n",
    "            since the multiprocessing module is not compatible with the pickle module. So we need to make the parallelization\n",
    "            explicitly in a process-oriented way.\n",
    "        \n",
    "            Params:\n",
    "            f: Input file. A file object.     \n",
    "               The input file for the whole HARP region, could be a numpy .npy file, or a .hdf5 file.\n",
    "               For npy file, its shape should be [num_frames,height,width]\n",
    "               For hdf5 file, its directory structure should be 'video0'->'frameXXX'->'channels'-> the Bz Image\n",
    "            \n",
    "            core: The number of processing cores for doing the parallalization.\n",
    "            \n",
    "            global_threshold: The percentile threshold used for selecting PILs throughout the whole video. Range from 0 to 1.\n",
    "                              When each image of the file has its PILs founded, all PILs throughout the video are compared\n",
    "                              based on their corresponding average gradient. And the quantile of the average gradient at\n",
    "                              the global_threshold is used to delete those weak PILs that have an average gradient that is\n",
    "                              below the quantile. The smaller it is, the more PILs will be retained for each image.\n",
    "            \n",
    "            edge_retain, polar_threshold, radius, min_samples, select, window_size: see the PIL class documentation.\n",
    "            \n",
    "            filename: The filename for saving the output file. No extensions needed.\n",
    "            \n",
    "            Output:\n",
    "            PIL_file: In case of a numpy array input, output a pickled dictionary. In cased of hdf5 file, output a corresponding\n",
    "                      hdf5. Saved in the same folder.                      \n",
    "        '''\n",
    "    \n",
    "    pool = mp.Pool(processes=core)\n",
    "    process_input = []\n",
    "    global_pos, global_neg = 0, 0\n",
    "    \n",
    "    if mode==\"video\":\n",
    "        PIL_detector = PIL(edge_retain=edge_retain,polar_threshold=polar_threshold,radius=radius,min_samples=min_samples,mode=mode)\n",
    "        global_pos, global_neg = PIL_detector.video_data_decompose(f)\n",
    "        if PIL_detector.invalid_video==True:\n",
    "            raise Exception(\"The video is not valid, either has too many missing values, or does not have positive/negative polars.\")\n",
    "    \n",
    "    if type(f)==np.ndarray:\n",
    "        for i in range(f.shape[0]):\n",
    "            the_input = [f[i],edge_retain,polar_threshold,radius,min_samples,select,window_size,i,mode,global_pos,global_neg]\n",
    "            process_input.append(the_input)\n",
    "        \n",
    "    if type(f)==h5py._hl.files.File:\n",
    "        # in case of an hdf5 input, output an hdf5 file with the same group structure \n",
    "        video = f['video0']\n",
    "        framelist = list(video.keys())\n",
    "        frames = sorted(framelist, key=lambda x: int(x[5:]), reverse=False) # order all the frames chronologically\n",
    "        images = [video[p]['channels'][:,:,2] for p in frames] # a list of input images\n",
    "        process_input = []\n",
    "\n",
    "        for i in range(len(frames)):\n",
    "            the_input = [images[i],edge_retain,polar_threshold,radius,min_samples,select,window_size,frames[i]]\n",
    "            process_input.append(the_input) \n",
    "    \n",
    "    # parallel training        \n",
    "    results = [pool.apply_async(PIL_fit,t) for t in process_input]\n",
    "    output = [p.get() for p in results]\n",
    "    \n",
    "    # compare all frames' PIL segments' average gradient and delete those PILs with very small gradient\n",
    "    all_gradient = [] # a list of PILs' gradients\n",
    "        \n",
    "    for item in output:\n",
    "        PIL_segment = item[0] # retain the result from an image\n",
    "        if len(PIL_segment.keys())>0: # check if any PILs are found in the image\n",
    "            for segment in PIL_segment:\n",
    "                all_gradient.append(PIL_segment[segment]['weight']) # store the weight value in all_gradient so as to do global thresholding on all images.\n",
    "        \n",
    "    threshold = np.percentile(np.array(all_gradient),q=100*global_threshold) # this threshold is to cut off those weak PILs\n",
    "\n",
    "    \n",
    "    # create output file\n",
    "    if type(f)==np.ndarray:  \n",
    "        final_result = defaultdict(dict)        \n",
    "        for item in output:\n",
    "            thekey = item[1]\n",
    "            PIL_segment = item[0]\n",
    "            if len(PIL_segment.keys())==0: # if no PIL for the image\n",
    "                final_result[thekey] = PIL_segment # store the result in a dictionary\n",
    "            else:\n",
    "                L = list(PIL_segment)\n",
    "                for line in L: # if there is any PIL for the image\n",
    "                    if PIL_segment[line]['weight']>threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        del PIL_segment[line] # if the gradient for the PIL is too small, throw the PIL away\n",
    "                \n",
    "                final_result[thekey] = PIL_segment # store the result in a dictionary\n",
    "            \n",
    "        with open(filename + 'PIL.pkl', 'wb') as f:\n",
    "            pickle.dump(final_result, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Output Created\")\n",
    "        \n",
    "    \n",
    "    if type(f)==h5py._hl.files.File:    \n",
    "        # create the new hdf5 file with the same structure\n",
    "        filename = filename+'PIL.hdf5'\n",
    "        PILfile = h5py.File(filename,mode='w')\n",
    "        PILfile.create_group(name='video0')\n",
    "        newvideo = PILfile['video0']\n",
    "        \n",
    "        for i in range(len(output)):\n",
    "            framename = output[i][1]\n",
    "            newvideo.create_group(name=framename)\n",
    "            newframe = newvideo[framename]\n",
    "            \n",
    "            PIL_segment = output[i][0] # the PIL segment dictionary object\n",
    "            if len(PIL_segment)==0:\n",
    "                newframe.create_dataset(name='PIL',data=PIL_segment)\n",
    "            else:\n",
    "                L = list(PIL_segment)\n",
    "                for line in L: # if there is any PIL for the image\n",
    "                    if PIL_segment[line]['weight']>threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        del PIL_segment[line] # if the gradient for the PIL is too small, throw the PIL away\n",
    "                newframe.create_dataset(name='PIL',data=PIL_segment) # to access a certain frames' PIL: f['video0']['frameX']['PIL'][PIL_segment_num]['coor'/'weight']\n",
    "        PILfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = PIL(edge_retain=0.003,polar_threshold=0.50,radius=12,min_samples=5,mode='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 377, 744)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('HARP377.npy')\n",
    "test_data = data[800:810]\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_data',test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tool.fitFile(test_data,select=2,window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.fit(image,select=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {0: {'coor': array([[222, 441],\n",
       "                     [222, 442],\n",
       "                     [223, 441],\n",
       "                     ...,\n",
       "                     [259, 457],\n",
       "                     [259, 458],\n",
       "                     [259, 459]], dtype=int64), 'weight': 92.13377611488512}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.PIL_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_items = {k: image_result[k] for k in image_result if k in result and image_result[k] == result[k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AR11158PIL.pkl', 'rb') as f:\n",
    "    image_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIL_global(PIL):\n",
    "    ''' The previous class PIL() works on images. While it is very useful to locate the PIL for each image of interest,\n",
    "        it is not an ideal approach when we are dealing with a complete magnetic field history of a video for a HARP region.\n",
    "        More specifically, when we are trying to locate the PIL for each image of the video, we use a global_threshold \n",
    "        parameter in the PIL() class to compare all PILs found in each image altogether and then only keep a certain \n",
    "        percentage of all PILs for the whole video. But when generating candidate PILs, we still only work on images \n",
    "        individually. In this class of PIL_global(), we are going to work on videos directly, and preprocess input for \n",
    "        all images altogether, and finally locate PILs for all image altogether. Hopefully, the results will be improved. \n",
    "        Since this time, the strength of a polar in a certain Bz image is determined by comparing it with all polars in the\n",
    "        whole video instead of just other polars within the same image.\n",
    "        \n",
    "        Params:\n",
    "        edge_retain, radius and min_samples: see documentation for the .__init__() method of the PIL() class\n",
    "        \n",
    "        polar_threshold: The strong polar threshold. Range from 0 to 1. Float number.\n",
    "                         The quantile of the magnitude of the vertical component above which a pixel is considered as \n",
    "                         a strong polar. Basically, each video is decomposed into a positive video and negative video.\n",
    "                         For a positive video, each frame of it only contains the positive pixels for that frame in the \n",
    "                         original video and all negative pixels are coerced to 0. The same principle applies to the definition\n",
    "                         of the negative video. And for a positive video, all pixels of all frames are ranked altogether, and \n",
    "                         the strong positive pixels is defined as the pixels above the polar_threshold quantile in this\n",
    "                         video-level pixel rank.\n",
    "    '''\n",
    "    \n",
    "    def hdf5_to_ndarray(self,f):\n",
    "        ''' For other methods under this class, one deals data with a numpy-ndarray data format. For any hdf5 data input,\n",
    "            this method enables the user to convert the data format to numpy array.\n",
    "            \n",
    "            Params:\n",
    "            f: data for a single HARP region. hdf5 format.\n",
    "            \n",
    "            Output:\n",
    "            array_data: data for vertical component of the magnetic field. numpy.ndarray format.\n",
    "                        shape = [num_frames,height.width]\n",
    "        '''\n",
    "        array_data = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def video_data_preparation(self,image,window_size=5,time_stride=5):\n",
    "        '''Data preparation method, a submethod for the .fit() method. Can be updated with maxpooling3D layer\n",
    "           from the Keras package. Update may apply later.\n",
    "        \n",
    "           Params:\n",
    "           video: Input video. Numpy array.\n",
    "                  Shape = [video_len,height,width]\n",
    "           \n",
    "           window_size: The range of a pixel's neighborhood. Odd number integer. No smaller than 3.\n",
    "                        For each pixel in each input image of the video, we consider a window_size*window_size, \n",
    "                        5*5 by default, subimage with the pixel of interest being at the center as its spatial \n",
    "                        neighborhood in the image. \n",
    "                        \n",
    "           time_stride: The range of a pixel's neighborhood in the time dimension. Odd number integer. No smaller than 1.\n",
    "                        For each pixel in the video, apart from its spatial neighborhood, \n",
    "                        \n",
    "           Output:\n",
    "           image_new: Output image ready for edge detection. Numpy array.\n",
    "                      Shape = [height, width], same as the input image.\n",
    "                      \n",
    "                      For each pixel in the original image, one consider the local maximum and local minimum of the pixel within\n",
    "                      its neighborhood. If both the local maximum and minimum passed the quantile threshold sepecified by \n",
    "                      self.polar_threshold in the corresponding positive and negative image, the pixel's value is retained,\n",
    "                      otherwise the pixel is put to zero.\n",
    "                      \n",
    "                      As a result, the output image only contains those pixels with both a strong positive and a strong negative\n",
    "                      polar within its neighborhood. This helps exclude those pixels with a strong positive/negative polar but \n",
    "                      no associative polar of the opposite polarity. So these pixels in the output image is a superset of PIL pixels.\n",
    "        '''    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
